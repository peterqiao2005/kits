version: '3.8'

services:
  vllm_1:
    image: vllm/vllm-openai:v0.6.3
    container_name: vllm_1
    runtime: nvidia
    deploy:
      resources:
        reservations:
          devices:
            - capabilities: ["gpu"]
    volumes:
      - ~/.cache/huggingface:/root/.cache/huggingface
    ports:
      - "8000:8000"
    ipc: "host"
    environment:
      - NCCL_NVLS_ENABLE=0
      - VLLM_API_KEY=sk-DtT3YuhbLJ8NLc7n730cFbA584794b5890C30e173859A00e
    command:
      - --model
      - unsloth/Meta-Llama-3.1-8B-Instruct
      - --tokenizer
      - "tau-vision/llama-tokenizer-fix"
      - --max_model_len
      - "20000"
      - --gpu_memory_utilization
      - "0.9"
      - --port
      - "8000"
      - --tensor-parallel-size
      - "8"
    restart: always

  vllm_2:
    image: vllm/vllm-openai:v0.6.3
    container_name: vllm_2
    runtime: nvidia
    deploy:
      resources:
        reservations:
          devices:
            - capabilities: ["gpu"]
    volumes:
      - ~/.cache/huggingface:/root/.cache/huggingface
    ports:
      - "8001:8001"
    ipc: "host"
    environment:
      - NCCL_NVLS_ENABLE=0
      - VLLM_API_KEY=sk-DtT3YuhbLJ8NLc7n730cFbA584794b5890C30e173859A00e
    command:
      - --model
      - hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4
      - --tokenizer
      - "tau-vision/llama-tokenizer-fix"
      - --max_model_len
      - "16000"
      - --gpu_memory_utilization
      - "0.9"
      - --port
      - "8001"
      - --tensor-parallel-size
      - "8"
    restart: always

  vllm_3:
    image: vllm/vllm-openai:v0.6.3
    container_name: vllm_3
    runtime: nvidia
    deploy:
      resources:
        reservations:
          devices:
            - capabilities: ["gpu"]
    volumes:
      - ~/.cache/huggingface:/root/.cache/huggingface
    ports:
      - "8002:8002"
    ipc: "host"
    environment:
      - NCCL_NVLS_ENABLE=0
      - VLLM_API_KEY=sk-DtT3YuhbLJ8NLc7n730cFbA584794b5890C30e173859A00e
    command:
      - --model
      - unsloth/Llama-3.2-3B-Instruct
      - --tokenizer
      - "tau-vision/llama-tokenizer-fix"
      - --max_model_len
      - "20000"
      - --gpu_memory_utilization
      - "0.9"
      - --port
      - "8002"
      - --tensor-parallel-size
      - "8"
    restart: always
  
  vllm_4:
    image: vllm/vllm-openai:v0.6.3
    container_name: vllm_4
    runtime: nvidia
    deploy:
      resources:
        reservations:
          devices:
            - capabilities: ["gpu"]
    volumes:
      - ~/.cache/huggingface:/root/.cache/huggingface
    ports:
      - "8003:8003"
    ipc: "host"
    environment:
      - NCCL_NVLS_ENABLE=0
      - VLLM_API_KEY=sk-DtT3YuhbLJ8NLc7n730cFbA584794b5890C30e173859A00e
    command:
      - --model
      - TheBloke/Rogue-Rose-103b-v0.2-AWQ
      - --tokenizer
      - "TheBloke/Rogue-Rose-103b-v0.2-AWQ"
      - --max_model_len
      - "4096"
      - --gpu_memory_utilization
      - "0.9"
      - --port
      - "8003"
      - --tensor-parallel-size
      - "8"
    restart: always
  
  vllm_5:
    image: vllm/vllm-openai:v0.6.3
    container_name: vllm_5
    runtime: nvidia
    deploy:
      resources:
        reservations:
          devices:
            - capabilities: ["gpu"]
    volumes:
      - ~/.cache/huggingface:/root/.cache/huggingface
    ports:
      - "8004:8004"
    ipc: "host"
    environment:
      - NCCL_NVLS_ENABLE=0
      - VLLM_API_KEY=sk-DtT3YuhbLJ8NLc7n730cFbA584794b5890C30e173859A00e
    command:
      - --model
      - casperhansen/deepseek-r1-distill-qwen-32b-awq
      - --tokenizer
      - "casperhansen/deepseek-r1-distill-qwen-32b-awq"
      - --max_model_len
      - "16000"
      - --gpu_memory_utilization
      - "0.9"
      - --port
      - "8004"
      - --tensor-parallel-size
      - "8"
    restart: always